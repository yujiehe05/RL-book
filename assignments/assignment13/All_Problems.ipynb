{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages needed\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Iterable, Callable, Mapping, TypeVar, List, Tuple, Optional,Sequence\n",
    "from rl.markov_decision_process import TransitionStep, Policy,MarkovDecisionProcess\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess,policy_from_q\n",
    "from rl.returns import returns\n",
    "from rl.function_approx import Tabular, FunctionApprox\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.iterate import last\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap, InventoryState\n",
    "from rl.chapter7.asset_alloc_discrete import AssetAllocDiscrete \n",
    "from rl.monte_carlo import mc_prediction, mc_control\n",
    "from rl.td import td_prediction\n",
    "\n",
    "from rl.distribution import Constant,Choose,Bernoulli,Distribution,Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular first: consulted RL-book implementation\n",
    "def policy_from_q_st(q: Mapping[Tuple[S,A],float],\n",
    "                     mdp: MarkovDecisionProcess[S, A],\n",
    "                     eps: float = 0.0) -> Policy[S, A]:\n",
    "    \n",
    "    explore = Bernoulli(eps)\n",
    "\n",
    "    class QPolicy(Policy[S, A]):\n",
    "        def act(self, s: S) -> Optional[Distribution[A]]:\n",
    "            if mdp.is_terminal(s):\n",
    "                return None\n",
    "\n",
    "            if explore.sample():\n",
    "                return Choose(set(mdp.actions(s)))\n",
    "\n",
    "            greedy = None\n",
    "            max_q = -np.Inf\n",
    "            for k in q:\n",
    "                if k[0] == s and q[k] > max_q:\n",
    "                    max_q = q[k]\n",
    "                    greedy = k[1]\n",
    "            return Constant(greedy)\n",
    "\n",
    "    return QPolicy()\n",
    "\n",
    "def tab_mc_control(mdp: MarkovDecisionProcess[S, A],\n",
    "                   states: Distribution[S],\n",
    "                   weight_func: Callable[[int],float],\n",
    "                   gamma: float,\n",
    "                   tol: float = 1e-6,\n",
    "                   maxIter: int = 10000) -> List[Mapping[Tuple[S,A],float]]:\n",
    "    \n",
    "    curr_q:Mapping[Tuple[S,A],float] = defaultdict(int)\n",
    "    q: List[Mapping[Tuple[S,A],float]] = [curr_q]\n",
    "    p: Policy[S,A] = policy_from_q_st(q, mdp,1.)\n",
    "    occurence: Mapping[Tuple[S,A],int] = defaultdict(int)\n",
    "\n",
    "    for n in range(maxIter):\n",
    "        trace: Iterable[TransitionStep[S, A]] = mdp.simulate_actions(states, p)\n",
    "        episode = returns(trace,gamma,tol)\n",
    "        for st in episode:\n",
    "            occurence[(st.state,st.action)] += 1\n",
    "            curr_q[(st.state,st.action)] = curr_q[(st.state,st.action)]*(1-\\\n",
    "                weight_func(occurence[(st.state,st.action)])) +\\\n",
    "                weight_func(occurence[(st.state,st.action)])*st.return_\n",
    "        q.append(curr_q)\n",
    "        p = policy_from_q_st(q[-1], mdp, 1./(n+2))\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test correctness\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = tab_mc_control(si_mdp,Choose(si_mdp.non_terminal_states),lambda n: 1./n,user_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_policy = defaultdict(lambda:-1)\n",
    "opt_vf = defaultdict(lambda:-100)\n",
    "for k in res_1[-1]:\n",
    "    if opt_vf[k[0]] < res_1[-1][k]:\n",
    "        opt_vf[k[0]] = res_1[-1][k]\n",
    "        opt_policy[k[0]] = k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = policy_iteration_result(si_mdp,user_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State InventoryState(on_hand=1, on_order=0): -28.9119 vs. -28.6610\n",
      "State InventoryState(on_hand=0, on_order=1): -27.9186 vs. -27.6610\n",
      "State InventoryState(on_hand=0, on_order=0): -35.5073 vs. -34.8948\n",
      "State InventoryState(on_hand=1, on_order=1): -29.3131 vs. -28.9919\n",
      "State InventoryState(on_hand=0, on_order=2): -28.3397 vs. -27.9919\n",
      "State InventoryState(on_hand=2, on_order=0): -30.3331 vs. -29.9919\n",
      "State InventoryState(on_hand=1, on_order=0): Action 1 vs. 1\n",
      "State InventoryState(on_hand=0, on_order=1): Action 1 vs. 1\n",
      "State InventoryState(on_hand=0, on_order=0): Action 2 vs. 1\n",
      "State InventoryState(on_hand=1, on_order=1): Action 0 vs. 0\n",
      "State InventoryState(on_hand=0, on_order=2): Action 0 vs. 0\n",
      "State InventoryState(on_hand=2, on_order=0): Action 0 vs. 0\n"
     ]
    }
   ],
   "source": [
    "for k in opt_vf:\n",
    "    print(f\"State {k}: %.4f vs. %.4f\"%(opt_vf[k],res_2[0][k]))\n",
    "for k in opt_policy:\n",
    "    print(f\"State {k}: Action %d vs. %d\"%(opt_policy[k],res_2[1].policy_map[k].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.tab_mc_control.<locals>.<lambda>()>,\n",
       "            {(InventoryState(on_hand=1, on_order=0), 0): -35.994149875802826,\n",
       "             (InventoryState(on_hand=1, on_order=0), 1): -28.91185152063077,\n",
       "             (InventoryState(on_hand=0, on_order=1), 0): -34.94270166698971,\n",
       "             (InventoryState(on_hand=0, on_order=0), 1): -40.63495526273648,\n",
       "             (InventoryState(on_hand=0, on_order=1), 1): -27.918579723898635,\n",
       "             (InventoryState(on_hand=1, on_order=1), 0): -29.313114149099746,\n",
       "             (InventoryState(on_hand=0, on_order=0), 0): -45.95170888025482,\n",
       "             (InventoryState(on_hand=0, on_order=0), 2): -35.50732172722377,\n",
       "             (InventoryState(on_hand=0, on_order=2), 0): -28.33967608248426,\n",
       "             (InventoryState(on_hand=2, on_order=0), 0): -30.33308220164507})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:<br>\n",
    "Everything correct except InventoryState(on_hand=0, on_order=0), why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Approx: consulted RL-book implementation\n",
    "# TODO: design for AssetAllocDiscrete only!\n",
    "def mc_control(mdp: MarkovDecisionProcess[S, A],\n",
    "               states: Distribution[S],\n",
    "               approx_0: FunctionApprox[Tuple[S, A]],\n",
    "               gamma: float,\n",
    "               tol: float = 1e-6,\n",
    "               maxIter: int = 1000) -> List[Mapping[Tuple[S,A],float]]:\n",
    "    \n",
    "    curr_q:FunctionApprox[Tuple[S, A]] = approx_0\n",
    "    q: List[FunctionApprox[Tuple[S, A]]] = [curr_q]\n",
    "    p: Policy[S,A] = policy_from_q(q, mdp,1.)\n",
    "\n",
    "    for n in range(maxIter):\n",
    "        trace: Iterable[markov_decision_process.TransitionStep[S, A]] = mdp.simulate_actions(states, p)\n",
    "        print(next(trace))\n",
    "        curr_q = curr_q.update(\n",
    "            ((st.state, st.action), st.return_)\n",
    "            for st in returns(trace, gamma, tol)\n",
    "        )\n",
    "        q.append(curr_q)\n",
    "        p = markov_decision_process.policy_from_q(q[-1], mdp, 1/(n+2.))\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_var: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(\n",
    "    2 / 3 * base_alloc,\n",
    "    4 / 3 * base_alloc,\n",
    "    11\n",
    ")\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "    [\n",
    "        lambda _: 1.,\n",
    "        lambda w_x: w_x[0],\n",
    "        lambda w_x: w_x[1],\n",
    "        lambda w_x: w_x[1] * w_x[1]\n",
    "    ]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv=lambda y: -y\n",
    ")\n",
    "init_wealth_distr: Gaussian = Constant(init_wealth)\n",
    "\n",
    "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More to be done.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssetAllocMDPwrap(MarkovDecisionProcess[Tuple[float,int], float]):\n",
    "    \n",
    "    def __init__(self,assetalloc: AssetAllocDiscrete):\n",
    "#        self.assetalloc: AssetAllocDiscrete = assetalloc\n",
    "        self.alloc_choices: Sequence[float] = assetalloc.risky_alloc_choices\n",
    "        self.steps: int = assetalloc.time_steps()\n",
    "        self.utility_f: Callable[[float], float] = assetalloc.utility_func\n",
    "\n",
    "    def step(self,state: Tuple[float,int],action: float\n",
    "            ) -> Optional[Distribution[Tuple[Tuple[float,int], float]]]:\n",
    "\n",
    "        def sr_sampler_func(state=state,alloc=action) -> Tuple[Tuple[float,int], float]:\n",
    "            \n",
    "            wealth=state[0]\n",
    "            time = state[1]\n",
    "            \n",
    "            distr: Distribution[float] = self.assetalloc.risky_return_distributions[time]\n",
    "            rate: float = self.assetalloc.riskless_returns[time]\n",
    "            \n",
    "            next_wealth: float = alloc * (1 + distr.sample()) \\\n",
    "                + (wealth - alloc) * (1 + rate)\n",
    "            reward: float = self.utility_f(next_wealth) \\\n",
    "                if t == self.steps - 1 else 0.\n",
    "            return ((next_wealth,time+1), reward)\n",
    "\n",
    "            return SampledDistribution(\n",
    "                sampler=sr_sampler_func,\n",
    "                expectation_samples=1000\n",
    "            )\n",
    "\n",
    "    def actions(self, wealth: float) -> Sequence[float]:\n",
    "        return self.alloc_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "aad_wrapped = AssetAllocMDPwrap(aad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
