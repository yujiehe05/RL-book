{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages needed\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Iterable, Callable, Mapping, TypeVar, List, Tuple, Optional\n",
    "from rl.markov_decision_process import TransitionStep, Policy,MarkovDecisionProcess\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.returns import returns\n",
    "from rl.function_approx import Tabular, FunctionApprox\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.iterate import last\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap, InventoryState\n",
    "from rl.monte_carlo import mc_prediction, mc_control\n",
    "from rl.td import td_prediction\n",
    "\n",
    "from rl.distribution import Constant,Choose,Bernoulli,Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular first\n",
    "def policy_from_q(q: Mapping[Tuple[S,A],float],\n",
    "                  mdp: MarkovDecisionProcess[S, A],\n",
    "                  eps: float = 0.0) -> Policy[S, A]:\n",
    "    \n",
    "    explore = Bernoulli(eps)\n",
    "\n",
    "    class QPolicy(Policy[S, A]):\n",
    "        def act(self, s: S) -> Optional[Distribution[A]]:\n",
    "            if mdp.is_terminal(s):\n",
    "                return None\n",
    "\n",
    "            if explore.sample():\n",
    "                return Choose(set(mdp.actions(s)))\n",
    "\n",
    "            greedy = None\n",
    "            max_q = -np.Inf\n",
    "            for k in q:\n",
    "                if k[0] == s and q[k] > max_q:\n",
    "                    max_q = q[k]\n",
    "                    greedy = k[1]\n",
    "            return Constant(greedy)\n",
    "\n",
    "    return QPolicy()\n",
    "\n",
    "def tab_mc_control(mdp: MarkovDecisionProcess[S, A],\n",
    "                   states: Distribution[S],\n",
    "                   weight_func: Callable[[int],float],\n",
    "                   gamma: float,\n",
    "                   tol: float = 1e-6,\n",
    "                   maxIter: int = 1000) -> List[Mapping[Tuple[S,A],float]]:\n",
    "    \n",
    "    curr_q:Mapping[Tuple[S,A],float] = defaultdict(lambda:0)\n",
    "    q: List[Mapping[Tuple[S,A],float]] = [curr_q]\n",
    "    p: Policy[S,A] = policy_from_q(q, mdp,1.)\n",
    "    occurence: Mapping[Tuple[S,A],int] = defaultdict(lambda:0)\n",
    "\n",
    "    for n in range(maxIter):\n",
    "        trace: Iterable[TransitionStep[S, A]] = mdp.simulate_actions(states, p)\n",
    "        episode = returns(trace,gamma,tol)\n",
    "        for st in episode:\n",
    "            occurence[(st.state,st.action)] += 1\n",
    "            curr_q[(st.state,st.action)] = curr_q[(st.state,st.action)]*(1-\\\n",
    "                weight_func(occurence[(st.state,st.action)])) +\\\n",
    "                weight_func(occurence[(st.state,st.action)])*st.return_\n",
    "        q.append(curr_q)\n",
    "        p = policy_from_q(q[-1], mdp, 1./(n+2))\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test correctness\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = tab_mc_control(si_mdp,Choose(si_mdp.non_terminal_states),lambda n: 1./n,user_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_policy = defaultdict(lambda:-1)\n",
    "opt_vf = defaultdict(lambda:-100)\n",
    "for k in res_1[-1]:\n",
    "    if opt_vf[k[0]] < res_1[-1][k]:\n",
    "        opt_vf[k[0]] = res_1[-1][k]\n",
    "        opt_policy[k[0]] = k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = policy_iteration_result(si_mdp,user_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State InventoryState(on_hand=1, on_order=0): -28.9862 vs. -28.6610\n",
      "State InventoryState(on_hand=0, on_order=0): -35.5641 vs. -34.8948\n",
      "State InventoryState(on_hand=0, on_order=2): -28.4044 vs. -27.9919\n",
      "State InventoryState(on_hand=0, on_order=1): -27.9477 vs. -27.6610\n",
      "State InventoryState(on_hand=2, on_order=0): -30.4252 vs. -29.9919\n",
      "State InventoryState(on_hand=1, on_order=1): -29.3745 vs. -28.9919\n",
      "State InventoryState(on_hand=1, on_order=0): Action 1 vs. 1\n",
      "State InventoryState(on_hand=0, on_order=0): Action 2 vs. 1\n",
      "State InventoryState(on_hand=0, on_order=2): Action 0 vs. 0\n",
      "State InventoryState(on_hand=0, on_order=1): Action 1 vs. 1\n",
      "State InventoryState(on_hand=2, on_order=0): Action 0 vs. 0\n",
      "State InventoryState(on_hand=1, on_order=1): Action 0 vs. 0\n"
     ]
    }
   ],
   "source": [
    "for k in opt_vf:\n",
    "    print(f\"State {k}: %.4f vs. %.4f\"%(opt_vf[k],res_2[0][k]))\n",
    "for k in opt_policy:\n",
    "    print(f\"State {k}: Action %d vs. %d\"%(opt_policy[k],res_2[1].policy_map[k].value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:<br>\n",
    "Everything correct except InventoryState(on_hand=0, on_order=0), why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
