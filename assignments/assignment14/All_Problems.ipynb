{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages needed\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import (Iterable, Callable, Mapping, TypeVar, \n",
    "                    List, Tuple, Optional,Sequence, Iterator)\n",
    "from rl.markov_decision_process import Policy,MarkovDecisionProcess\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess,policy_from_q\n",
    "from rl.markov_decision_process import TransitionStep as TransitionStepA\n",
    "from rl.markov_process import TransitionStep\n",
    "from rl.returns import returns\n",
    "from rl.function_approx import Tabular, FunctionApprox\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.iterate import last\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import replace\n",
    "\n",
    "from rl.chapter2.simple_inventory_mrp import SimpleInventoryMRPFinite, InventoryState\n",
    "from rl.chapter7.asset_alloc_discrete import AssetAllocDiscrete \n",
    "from rl.monte_carlo import mc_prediction, mc_control\n",
    "from rl.td import td_prediction\n",
    "\n",
    "from rl.distribution import Constant,Choose,Bernoulli,Distribution,Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTD(trans: Iterable[TransitionStep[S]],\n",
    "         feature_functions: Sequence[Callable[[S], float]],\n",
    "         gamma: float)-> Mapping[S,float]:\n",
    "    \n",
    "    m:int = len(feature_functions)\n",
    "    A:np.ndarray = np.zeros((m,m))\n",
    "    b:np.ndarray = np.zeros((m,1))\n",
    "    v:Mapping[S,float] = {}\n",
    "    \n",
    "    for s in trans:\n",
    "        if s.state not in v:\n",
    "            v[s.state] = 0.0\n",
    "        feature_val_s:np.ndarray = np.reshape(np.array([f(s.state) for f in feature_functions]),(m,1))\n",
    "        feature_val_ns:np.ndarray = np.reshape(np.array([f(s.next_state) for f in feature_functions]),(m,1))\n",
    "        A = A+np.dot(feature_val_s,(feature_val_s-gamma*feature_val_ns).T)\n",
    "        b = b+feature_val_s*s.reward\n",
    "        \n",
    "    w_star:np.ndarray = np.dot(np.linalg.inv(A),b)\n",
    "    for k in v:\n",
    "        feature_val_s:np.ndarray = np.reshape(np.array([f(k) for f in feature_functions]),(m,1))\n",
    "        v[s] = np.dot(feature_val_s.T,w_star)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSPI(trans: Iterable[TransitionStepA[S,A]],\n",
    "         feature_functions: Sequence[Callable[[S,A], float]],\n",
    "         gamma: float)-> Mapping[Tuple[S,A],float]:\n",
    "    \n",
    "    m:int = len(approx.feature_functions)\n",
    "    Ainv:np.ndarray = np.zeros((m,m))\n",
    "    A:np.ndarray = np.zeros((m,m))\n",
    "    b:np.ndarray = np.zeros((m,1))\n",
    "    q:Mapping[S,Mapping[A,float]] = {}\n",
    "        \n",
    "    def find_a(s:S, q:Mapping[S,Mapping[A,float]]) -> A:\n",
    "        best_vf: float = -np.inf\n",
    "        best_act: A = None\n",
    "        for a in q[s]:\n",
    "            if q[s][a] > best_vf:\n",
    "                best_vf = q[s][a]\n",
    "                best_act = a\n",
    "                \n",
    "        return best_act            \n",
    "    \n",
    "    count = 0\n",
    "    for s in trans:\n",
    "        if s.state not in q:\n",
    "            q[s.state] = {}\n",
    "            q[s.state][s.action] = 0.0\n",
    "        elif s.action not in q[s.state]:\n",
    "            q[s.state][s.action] = 0.0\n",
    "        u:np.ndarray = np.reshape(np.array([f(s.state,s.action) for f in approx.feature_functions]),(m,1))\n",
    "        best_act = find_a(s.next_state,q)\n",
    "        if best_act is None:\n",
    "            Ainv = Ainv-np.dot(np.dot(Ainv,np.dot(u,u.T)),Ainv)/(1+np.dot(np.dot(u.T,Ainv),u))\n",
    "        else:\n",
    "            v:np.ndarray = np.reshape(np.array([f(s.next_state,best_act) \n",
    "                                                for f in approx.feature_functions]),(m,1))\n",
    "            Ainv = Ainv-np.dot(np.dot(Ainv,np.dot(u,v.T)),Ainv)/(1+np.dot(np.dot(v.T,Ainv),u))\n",
    "        b = b+u*s.reward\n",
    "        if count == 0:\n",
    "            Ainv = np.linalg.inv(A)\n",
    "            w_star:np.ndarray = np.dot(Ainv,b)\n",
    "            count += 1\n",
    "        else:\n",
    "            w_star:np.ndarray = np.dot(Ainv,b)\n",
    "                \n",
    "        for st in q:\n",
    "            for a in q[st]:\n",
    "                q[st][a] = np.dot(u.T,w_star)\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
