{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frog on Lilypad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import section - use the abstract Markov Process class from rl directory\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess, StateActionMapping, FinitePolicy\n",
    "from rl.distribution import Categorical, Constant\n",
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3.2\n",
    "Assumption: user-defined n is between 2 and 100. Action representation: 0 = \"A\", 1 = \"B\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrogEscape(FiniteMarkovDecisionProcess[int,int]):\n",
    "    \n",
    "    def __init__(self,n:int):\n",
    "        self.n = max(min(n,100),2)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "        \n",
    "    def get_action_transition_reward_map(self) -> StateActionMapping[int,int]:\n",
    "        d: Dict[int,Dict[str,Categorical[Tuple[int,float]]]] = {}\n",
    "            \n",
    "        d[0] = None\n",
    "        d[self.n] = None\n",
    "        \n",
    "        for s in range(1,self.n):\n",
    "            a: Dict[str,Categorical[Tuple[int,float]]] = {}\n",
    "            a[0] = Categorical({(s-1,-1.):(float(s)/float(self.n)),\n",
    "                               (s+1,1.):(1-float(s)/float(self.n))})\n",
    "            a[1] = Categorical({(i,float(i-s)):(1./self.n)\n",
    "                                 for i in range(self.n+1)})\n",
    "            d[s] = a\n",
    "            \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the below-used of algorithm, see \"Theoretical-Hand\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_deterministic(n:int) -> Tuple[np.ndarray, FinitePolicy[int,int]]:\n",
    "    fe: FrogEscape = FrogEscape(n = n)\n",
    "    \n",
    "    totalNode: int = np.array([2**i for i in range(1,n)]).sum()\n",
    "    leaveStart: int = totalNode - 2**(n-1)\n",
    "        \n",
    "    optVF: np.ndarray = np.repeat(-np.inf,n-1)\n",
    "    optPolicy: FinitePolicy[int,int] = None\n",
    "    \n",
    "    for leaves in range(leaveStart,totalNode):\n",
    "        actionList: list = [None]\n",
    "        if leaves % 2 == 0:\n",
    "            actionList.append(0)\n",
    "        else:\n",
    "            actionList.append(1)\n",
    "        for _ in range(n-2):\n",
    "            if (math.floor(leaves/2)-1) % 2 == 0:\n",
    "                actionList.append(0)\n",
    "            else:\n",
    "                actionList.append(1)\n",
    "            leaves = math.floor(leaves/2)-1\n",
    "        actionList.append(None)\n",
    "        \n",
    "        currPolicy: FinitePolicy[int,int] = FinitePolicy({i:Constant(actionList[i]) for i in range(n+1)})\n",
    "        \n",
    "        implied_fe: FiniteMarkovRewardProcess[int] = fe.apply_finite_policy(currPolicy)\n",
    "        currVF: np.ndarray = implied_fe.get_value_function_vec(gamma = 1.)\n",
    "            \n",
    "        if np.all(currVF > optVF):\n",
    "            optVF = currVF\n",
    "            optPolicy = currPolicy\n",
    "    \n",
    "    if optPolicy == None:\n",
    "        return\n",
    "    else:\n",
    "        return optVF,optPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.71428571, 0.14285714]),\n",
       " For State 0:\n",
       "   Do Action None with Probability 1.000\n",
       " For State 1:\n",
       "   Do Action 1 with Probability 1.000\n",
       " For State 2:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 3:\n",
       "   Do Action None with Probability 1.000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Correctness\n",
    "optimal_deterministic(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.95744681,  2.21276596,  1.34042553,  0.46808511, -0.27659574]),\n",
       " For State 0:\n",
       "   Do Action None with Probability 1.000\n",
       " For State 1:\n",
       "   Do Action 1 with Probability 1.000\n",
       " For State 2:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 3:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 4:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 5:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 6:\n",
       "   Do Action None with Probability 1.000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Correctness\n",
    "optimal_deterministic(n = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.08108108,  4.34712838,  3.42314189,  2.46114865,  1.49155405,\n",
       "         0.52956081, -0.39442568, -1.12837838]),\n",
       " For State 0:\n",
       "   Do Action None with Probability 1.000\n",
       " For State 1:\n",
       "   Do Action 1 with Probability 1.000\n",
       " For State 2:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 3:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 4:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 5:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 6:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 7:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 8:\n",
       "   Do Action 0 with Probability 1.000\n",
       " For State 9:\n",
       "   Do Action None with Probability 1.000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Correctness\n",
    "optimal_deterministic(n = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
