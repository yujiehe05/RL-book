{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages needed\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import (Iterable, Callable, Mapping, TypeVar, \n",
    "                    List, Tuple, Optional,Sequence, Iterator)\n",
    "from rl.markov_process import MarkovRewardProcess\n",
    "from rl.markov_decision_process import Policy,MarkovDecisionProcess,FinitePolicy\n",
    "from rl.function_approx import Tabular, FunctionApprox\n",
    "from rl.dynamic_programming import policy_iteration_result,value_iteration_result\n",
    "from rl.iterate import last,iterate\n",
    "from rl.approximate_dynamic_programming import evaluate_mrp, value_iteration\n",
    "\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap, InventoryState\n",
    "from rl.chapter7.asset_alloc_discrete import AssetAllocDiscrete \n",
    "from rl.monte_carlo import mc_prediction, mc_control\n",
    "from rl.td import td_prediction\n",
    "\n",
    "from rl.distribution import Constant,Distribution,Choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_from_v(\n",
    "        v: FunctionApprox[S],\n",
    "        mdp: MarkovDecisionProcess[S, A]\n",
    ") -> Policy[S, A]:\n",
    "    '''Return a policy that chooses the action that maximizes the reward\n",
    "    for each state in the given V function.\n",
    "    Arguments:\n",
    "      v -- approximation of the V function for the MDP\n",
    "      mdp -- the process for which we're generating a policy\n",
    "    Returns a greedy policy based on the given V function.\n",
    "    '''\n",
    "\n",
    "    class VPolicy(Policy[S, A]):\n",
    "        def act(self, s: S) -> Optional[Distribution[A]]:\n",
    "            if mdp.is_terminal(s):\n",
    "                return None\n",
    "\n",
    "            action = v.argmax(mdp.actions(s))\n",
    "            return Constant(action)\n",
    "\n",
    "    return VPolicy()\n",
    "\n",
    "def policy_iteration(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    gamma: float,\n",
    "    fa: FunctionApprox[S],\n",
    "    non_terminal: Distribution[S],\n",
    "    num_state = 1000) -> Iterator[Tuple[FunctionApprox[S], Policy[S, A]]]:\n",
    "    '''Calculate the value function (V*) of the given MDP by improving\n",
    "    the policy repeatedly after evaluating the value function for a policy\n",
    "    '''\n",
    "\n",
    "    def update(vf_policy: Tuple[FunctionApprox[S], Policy[S, A]])-> Tuple[FunctionApprox[S], Policy[S, A]]:\n",
    "\n",
    "        vf, pi = vf_policy\n",
    "        mrp: MarkovRewardProcess[S] = mdp.apply_policy(pi)\n",
    "        policy_vf: FunctionApprox[S] = last(evaluate_mrp(mrp, gamma, vf, non_terminal, num_state))\n",
    "        improved_pi: Policy[S, A] = policy_from_v(policy_vf,mdp)\n",
    "\n",
    "        return policy_vf, improved_pi\n",
    "\n",
    "    pi_0 = policy_from_v(fa,mdp)\n",
    "    return iterate(update, (fa, pi_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Policy Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{InventoryState(on_hand=0, on_order=0): -34.89484641847035, InventoryState(on_hand=0, on_order=1): -27.660950868477816, InventoryState(on_hand=0, on_order=2): -27.991890728243845, InventoryState(on_hand=1, on_order=0): -28.660950868477816, InventoryState(on_hand=1, on_order=1): -28.991890728243845, InventoryState(on_hand=2, on_order=0): -29.991890728243845}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n",
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{InventoryState(on_hand=0, on_order=0): -34.89484576629397, InventoryState(on_hand=0, on_order=1): -27.66095021630144, InventoryState(on_hand=0, on_order=2): -27.991890076067463, InventoryState(on_hand=1, on_order=0): -28.660950216301437, InventoryState(on_hand=1, on_order=1): -28.991890076067467, InventoryState(on_hand=2, on_order=0): -29.991890076067463}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    "    )\n",
    "\n",
    "fdp: FinitePolicy[InventoryState, int] = FinitePolicy(\n",
    "    {InventoryState(alpha, beta):\n",
    "     Constant(user_capacity - (alpha + beta)) for alpha in\n",
    "     range(user_capacity + 1) for beta in range(user_capacity + 1 - alpha)}\n",
    ")\n",
    "\n",
    "print(\"MDP Policy Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\")\n",
    "opt_vf_pi, opt_policy_pi = policy_iteration_result(\n",
    "    si_mdp,\n",
    "    gamma=user_gamma\n",
    ")\n",
    "print(opt_vf_pi)\n",
    "print(opt_policy_pi)\n",
    "print()\n",
    "\n",
    "print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\")\n",
    "opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp, gamma=user_gamma)\n",
    "print(opt_vf_vi)\n",
    "print(opt_policy_vi)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = Tabular()\n",
    "nt = Choose(si_mdp.non_terminal_states)\n",
    "last(value_iteration(si_mdp,user_gamma,fa,nt,10))\n",
    "\n",
    "last(policy_iteration(si_mdp,user_gamma,fa,nt,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow and cannot get a result using approximate policy iteration and value iteration. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
